---
layout: post
title: To Citrics, or Not To Citrics
subtitle: From 0 to 100 on a Full-Scale Web Application and Data Science Project
bigimg: /img/
tags: [Data Analytics, Data Science, Research]

---

# Background and Insights
First, an introduction; hello, my name is Rourke Struthers and I am currently a student in the Data Science and Machine Learning track at Lambda School. Over the last couple of 
weeks I have been working on a coding project called Citrics -- a web application that allows users to explore and compare historical weather, job, and real estate data, as well 
as predictions for future trends in these areas, in specific cities around the continental United States that they may consider moving to. One of the main consumer problems this 
project is trying to solve is the inabiity to have concise and relevant data readily available, in user chosen categories that actually matter to you. When you are looking to 
move to a new city, what are some of the most important criterion to consider? Citrics helps not only to determine this, but also give you reliable predictions as to the overall 
growth and trends in those specified areas. For example, if you're a real estate employee trying to move from New York to either Chicago or Lake Tahoe, some of the major 
considerations would be weather, job market data and trends, as well as real estate values and the what the appreciation yoour property might experience over your desired time 
horizon -- Citrics helps to provde all of that relevant information and then some. The value proposition to users is not only the data and the predictions however, it is all 
encompassing of the entire experience when using the application itself -- the hope and desire is a product that can provide valuable insight, utility and research potential for 
any user with a technical background or otherwise.  

I was slightly concerned going into this project since it was the first one I have done of this size and scale, as well as the fact it is completely from scratch, but after 
working on it for the last couple of weeks I have completely assuaged those fears with my technical skill and competence. This project has shown me that I have the ability, 
potential and skill to be a great Data Scientist and Software Engineer -- which makes me very excited for my future.


# Features and Technical Challenges
My role on the Citrics project has been prodominantly focused on Data Analysis and Analytics, Manipulation, Research, and Modeling -- but I have also made effective 
contributions to the development and deployment of the Data Science API (deployed with FastAPI) on Amazon Web Services (AWS) using Docker and AWS Elatic Bean Stalk. The product 
I worked with most directly is the actual data itself; I ensure that all the data is collected, cleaned, organized and then stored in a database so that it can be accessed 
effeciently later on. So my responsibilities include accessing APIs to collect the data, writing functions and queries to clean and manipulate the data to get it into the right 
format, develop the data engineering storage schema, make decisions around the most important modeling questions -- specifically aroudn time-series data and the statistical 
concepts surrounding the data modeling, and much more. I would say concisely, that I own the Data Engineering as well as Data Exploration, Analysis, and Modeling parts of the 
Citrics project.

One of the main technical challenges we encountered was our inability to pass our graphical visualizations to the front-end team through our API and the Docker images that act 
as the medium for that process. This was a considerable issue for us given that if we are not able to pass along our visualizations, there is no real ability for the user to see
the comparisons between the cities they are viewing. What we were able to determine is that we needed a backend database that holds all of our information, and the functions we 
create to visualize this information need to be inserted into the API code itself so they can be created, rendered and push through the pipeline seemlessly. Getting a little 
more specific, if our data is not stored in a database, there is no ability for the API to access the information when necessary. This makes it imperative we setup a database if 
only to ensure our data pipeline is robust enough to handle all upstream or downstream queries. We discovered this was an issue only after creating comparison functions that 
create visualizations for the specified cities and tried to push the Docker images of the visualizations to the front-end. Without the diligent work of my teammates, we 
would probably still be trying to figure out possible solutions. Fortunately though, there is a lot of cognitive diversity on our team, which allows us to solve problems more 
easily since there are many different perspectives and processes to choose from for any given problem that arises.  

Another major technical challenge we faced was getting all the relevant city specific information aligned with the respective states -- this is critical because without the 
right features and alignment, the data will not make any sense. For example, if you are searching for Albany, New York but instead the data is misaligned and Albany, Georgia 
information is contained in the New York dataset, there is  no way to ensure the reliability of the information being presented. Which is the worst thing when trying to get 
people to use the porduct since they can't trust the reliability of the product to provide customer value. I was able to fix this by creating functions that organize the data by 
state, then city, and the respective values for each feature set. I provided the entire cleaning and wrangle function for our real estate dataset. 

python```


```








