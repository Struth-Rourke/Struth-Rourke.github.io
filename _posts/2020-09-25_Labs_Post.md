---
layout: post
title: To Citrics, or Not To Citrics
subtitle: From 0 to 100 on a Full-Scale Web Application and Data Science Project
bigimg: /img/
tags: [Data Analytics, Data Science, Research]

---

# Background and Insights
First, an introduction; hello, my name is Rourke Struthers and I am currently a student in the Data Science and Machine Learning track at Lambda School. Over the last couple of 
weeks I have been working on a coding project called Citrics -- a web application that allows users to explore and compare historical weather, job, and real estate data, as well 
as predictions for future trends in these areas, in specific cities around the continental United States that they may consider moving to. One of the main consumer problems this 
project is trying to solve is the inabiity to have concise and relevant data readily available, in user chosen categories that actually matter to you. When you are looking to 
move to a new city, what are some of the most important criterion to consider? Citrics helps not only to determine this, but also give you reliable predictions as to the overall 
growth and trends in those specified areas. For example, if you're a real estate employee trying to move from New York to either Chicago or Lake Tahoe, some of the major 
considerations would be weather, job market data and trends, as well as real estate values and the what the appreciation yoour property might experience over your desired time 
horizon -- Citrics helps to provde all of that relevant information and then some. The value proposition to users is not only the data and the predictions however, it is all 
encompassing of the entire experience when using the application itself -- the hope and desire is a product that can provide valuable insight, utility and research potential for 
any user with a technical background or otherwise.  

I was slightly concerned going into this project since it was the first one I have done of this size and scale, as well as the fact it is completely from scratch, but after 
working on it for the last couple of weeks I have completely assuaged those fears with my technical skill and competence. This project has shown me that I have the ability, 
potential and skill to be a great Data Scientist and Software Engineer -- which makes me very excited for my future.


# Responsibilities, Challenges and Replication
My role on the Citrics project has been prodominantly focused on Data Analysis and Analytics, Manipulation, Research, and Modeling -- but I have also made effective 
contributions to the development and deployment of the Data Science API (deployed with FastAPI) on Amazon Web Services (AWS) using Docker and AWS Elatic Bean Stalk. The product 
I worked with most directly is the actual data itself; I ensure that all the data is collected, cleaned, organized and then stored in a database so that it can be accessed 
effeciently later on. So my responsibilities include accessing APIs to collect the data, writing functions and queries to clean and manipulate the data to get it into the right 
format, develop the data engineering storage schema, make decisions around the most important modeling questions -- specifically aroudn time-series data and the statistical 
concepts surrounding the data modeling, and much more. I would say concisely, that I own the Data Engineering as well as Data Exploration, Analysis, and Modeling parts of the 
Citrics project.

One of the main technical challenges we encountered was our inability to pass our graphical visualizations to the front-end team through our API and the Docker images that act 
as the medium for that process. This was a considerable issue for us given that if we are not able to pass along our visualizations, there is no real ability for the user to see
the comparisons between the cities they are viewing. What we were able to determine is that we needed a backend database that holds all of our information, and the functions we 
create to visualize this information need to be inserted into the API code itself so they can be created, rendered and push through the pipeline seemlessly. Getting a little 
more specific, if our data is not stored in a database, there is no ability for the API to access the information when necessary. This makes it imperative we setup a database if 
only to ensure our data pipeline is robust enough to handle all upstream or downstream queries. We discovered this was an issue only after creating comparison functions that 
create visualizations for the specified cities and tried to push the Docker images of the visualizations to the front-end. Without the diligent work of my teammates, we 
would probably still be trying to figure out possible solutions. Fortunately though, there is a lot of cognitive diversity on our team, which allows us to solve problems more 
easily since there are many different perspectives and processes to choose from for any given problem that arises.  

Another major technical challenge we faced was getting all the relevant city specific information aligned with the respective states -- this is critical because without the 
right features and alignment, the data will not make any sense. For example, if you are searching for Albany, New York but instead the data is misaligned and Albany, Georgia 
information is contained in the New York dataset, there is  no way to ensure the reliability of the information being presented. Which is the worst thing when trying to get 
people to use the porduct since they can't trust the reliability of the product to provide customer value. I was able to fix this by creating functions that organize the data by 
state, then city, and the respective values for each feature set. I provided the entire cleaning and wrangle function for our weather dataset below -- this is just one instance 
and example of the functional code I have written to ensure that all these data processes are able to scale effectively to any number of cities or information. It is also a 
concrete example of how we have functionalized the code to make it as dynamic as possible, as well as replicable to any other person who views our working notebooks.   

```python
# Creating a single function for ALL the weather data
def weather_data_function(df):

  ## Preliminary cleaning function
  def convert_location_to_state_plus_postal(data):
    # us state abbr list for capitals with "-" in between cities
    us_state_abbrev = {'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ',
                       'Arkansas': 'AR', 'California': 'CA', 'Colorado': 'CO',
                       'Connecticut': 'CT', 'Delaware': 'DE', 'District-of-Columbia': 'DC',
                       'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI','Idaho': 'ID',
                       'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',
                       'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA',
                       'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA',
                       'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',
                       'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 
                       'Nevada': 'NV', 'New-Hampshire': 'NH', 'New-Jersey': 'NJ',
                       'New-Mexico': 'NM', 'New-York': 'NY', 'North-Carolina': 'NC',
                       'North-Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',
                       'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode-Island': 'RI',
                       'South-Carolina': 'SC', 'South-Dakota': 'SD', 'Tennessee': 'TN',
                       'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virginia': 'VA',
                       'Washington': 'WA', 'West-Virginia': 'WV', 'Wisconsin': 'WI',
                       'Wyoming': 'WY'}

    # split location between city and state and save to "new" df
    new = data["location"].str.split("_", n = 1, expand = True)

    # create "city" and "state" cols in data df based on "new" df
    data["city"] = new[0] 
    data["state"] = new[1]

    # map postals over state column and save to new postals column
    data["postal"] = data["state"].map(us_state_abbrev)

    # concat city + _ +  postal 
    data['location'] = data['city'] + "_" + data['postal']

    # drop unnecessary cols
    data = data.drop(['city', 'state', 'postal'], axis = 1)

    return data

  # Calling function
  data = convert_location_to_state_plus_postal(df)
  
  ## PART 1:
  # clean function
  def clean(DataFrame):

      # copy frame
      X = DataFrame.copy()

      # drop unneeded cols
      X = X.drop(['Unnamed: 0','winddirDegree','moonrise','moonset','moon_illumination','sunrise','sunset','sunHour','tempC'], axis=1)

      # clean out this data, since there are issues with it
      X = X[X['mintempC'] != 0]
      X = X[X['maxtempC'] != 0]

      # create temp df to str split location into multiple cols then concat new DF
      loc_temp = X['location'].str.split('_', expand=True)
      loc_temp = loc_temp.rename(columns={0: "city", 1: "state_abbr"})
      X = pd.concat([X, loc_temp], axis=1)
      X = X.drop('location', axis=1)

      # rename cols from str.split
      X = X.rename(columns={0: "city", 1: "state_abbr"})

      # convert new columns to strings
      X['city'] = X['city'].astype('string')
      X['state_abbr'] = X['state_abbr'].astype('string')

      # replace dashes with spaces in city col
      X['city'] = X['city'].str.replace('-', ' ')

      # split date_time into categories for splitting df seasonally
      X['date_time'] = pd.to_datetime(X['date_time'], infer_datetime_format=True)
      X['year'] = X['date_time'].dt.year
      X['month'] = X['date_time'].dt.month
      X['day'] = X['date_time'].dt.day

      # create cols for fahrenheit from temp_c cols
      X['maxtempF'] = X['maxtempC'] * (9/5) + 32
      X['mintempF'] = X['mintempC'] * (9/5) + 32
      X['FeelsLikeF'] = X['FeelsLikeC'] * (9/5) + 32

      # adding new column 
      X['city_state'] = X['city'] + ", " + X['state_abbr']

      # ordering the columns in the desired way
      cols = ['city', 'state_abbr', 'city_state', 'maxtempF', 'FeelsLikeF', 
              'mintempF', 'maxtempC', 'FeelsLikeC', 'mintempC', 'precipMM', 
              'totalSnow_cm', 'uvIndex', 'DewPointC', 'HeatIndexC', 'WindChillC', 
              'WindGustKmph', 'cloudcover', 'humidity', 'pressure', 'visibility', 
              'windspeedKmph', 'year', 'month', 'day', 'date_time']

      # setting column order
      X = X[cols]

      new_col_names = ['City', 'Postal', 'City_State', 'MaxTempF', 'FeelsLikeF', 
              'MinTempF', 'MaxTempC', 'FeelsLikeC', 'MinTempC', 'Precip_mm', 
              'TotalSnow_cm', 'UVindex', 'DewPointC', 'HeatIndexC', 'WindChillC', 
              'WindGust_kmph', 'CloudCover', 'Humidity', 'Pressure', 'Visibility', 
              'WindSpeed_kmph', 'year', 'month', 'day', 'date_time']

      cols_new_cols_dict = dict(zip(cols, new_col_names))

      X.rename(columns = cols_new_cols_dict, inplace=True)


      return X

  ## CALLING FUNCTION: clean
  # calling the clean function on the df 
  data = clean(df)


  ## PART 2:
  # Creating the final dictionary that will get converted to JSON that will hold
  # all the relevant weather data
  Final_Weather_Data_dict = {}

  for i in data['Postal']:
    if i not in Final_Weather_Data_dict:
      Final_Weather_Data_dict[i] = {}


  ## PART 3:
  # Creating a dictionary that holds the City_State as the key and the Postal
  # code as the value 
  def Postal_City_State_Weather_dict_func(df, Final_Weather_dict):

    # Grouping the data by City_State, and getting the column means
    City_State_groupby = df.groupby('City_State').mean()
    # Creating a list of the City_State_groupby_indices
    City_State_groupby_indices = City_State_groupby.index.to_list()

    # Instantiating a new dictionary to hold the City_States as a key, and the 
    # Postal codes as values
    City_State_Postal_dict = {}
    # Looping through the City_States_groupby_indices to create entries in the 
    # City_State_Postal_dict -- simultaneously adding the postal code snippet,
    # (ex."NY") from the final two characters in the City_States string i, to 
    # the value, at the specified key, in City_State_Postal_dict
    for i in City_State_groupby_indices:
      City_State_Postal_dict[i] = i[-2:]

    # Looping through the City_State_Postal_dict (keys: City_State)
    for i in City_State_Postal_dict:
      # If the (value: Postal) is in the Final_Weather_dict (keys: Postal)
      if City_State_Postal_dict[i] in Final_Weather_dict:
        # Set the Final_Weather_Dict (key: Postal) at the specified (value: City_State)
        # as the City_State
        Final_Weather_dict[City_State_Postal_dict[i]][i] = i

    # Return the Final_Weather_dict
    return Final_Weather_dict

  ## CALLING FUNCTION: Postal_City_State_Weather_dict_func
  Final_Weather_Data_dict = Postal_City_State_Weather_dict_func(data, Final_Weather_Data_dict)

  ## PART 4:
  # Split data into a summer_df and winter_df for the respective timeframes
  # summer_df = April to September
  summer_df = data[(data['month'] >= 4) & (data['month'] <= 9)]
  summer_df = summer_df.copy()
  # winter_df = October to March
  winter_df = data[(data['month'] < 4) | (data['month'] > 9)]
  winter_df = winter_df.copy()


  ## PART 5:
  # Creating a city_state_avg_dict that holds all the mean values, for the
  # respective columns, in the respective season
  summer_city_state_avg = summer_df.groupby('City_State').mean()
  summer_city_state_avg_dict = summer_city_state_avg.to_dict(orient='index')

  winter_city_state_avg = winter_df.groupby('City_State').mean()
  winter_city_state_avg_dict = winter_city_state_avg.to_dict(orient='index')


  ## PART 6:
  def Postal_City_State_Weather_dict_FINAL_func(df, Final_Weather_dict):
    # Instantiate an empty dictionary to hold the City_State as the key and the
    # Weather data as the value
    City_State_Weather_dict = {}

    # Looping through the City_State
    for j in df['City_State']:
      # If it's NOT IN the City_State_Weather_dict
      if j not in City_State_Weather_dict:
        # Instantiate an empty dictionary as the value in City_State_Weather_dict 
        City_State_Weather_dict[j] = {}

    # Looping through the City_State in the City_State_Weather_dict 
    for x in City_State_Weather_dict:
      # If it IS IN the summer_city_state_avg_dict 
      if x in summer_city_state_avg_dict:
        # Create an entry called 'summer' in the City_State_Weather_dict value
        # space which holds the weather data from the summer_city_state_avg_dict
        # at the specified City_State
        City_State_Weather_dict[x]['summer'] = summer_city_state_avg_dict[x]

    # Doing the same as above for the winter_city_state_avg_dict 
    for x in City_State_Weather_dict:
      if x in winter_city_state_avg_dict:
        City_State_Weather_dict[x]['winter'] = winter_city_state_avg_dict[x]

    # Looping through the postal codes in Final_Weather_dict  
    for postal in Final_Weather_dict:
      # Looping through the city_states in the postal dictionary
      for city_state in Final_Weather_dict[postal]:
        # If the city_state is in the City_State_Weather_dict 
        if city_state in City_State_Weather_dict:
          # Set the Final_Weather_dict value for the city_state, in the 
          # respective postal code, as the weather data with the respective seasons 
          Final_Weather_dict[postal][city_state] = City_State_Weather_dict[city_state]
    
    # Return the Final_Weather_Dict
    return Final_Weather_dict

  ## CALLING FUNCTION: Postal_City_State_Weather_dict_FINAL_func
  FINAL_Weather_dict = Postal_City_State_Weather_dict_FINAL_func(data, Final_Weather_Data_dict)

  # Return Postal_City_State_Weather_dict which will become a JSON 
  return FINAL_Weather_dict


## CHECK:
# Calling Function
weather_data = weather_data_function(data)
# Viewing
weather_data
```

I apologize for how lengthy this function was, but I think it is imperative that I provide an example of the code written to display an example of how some of the code was 
written, but more importantly to show the approach we have taken to this whole project. At scale, this is a very large project -- it incorporates more than 650 cities, over 150 
million rows of data (this is continually growing as we continue to add more features and/or datasets to supplement the baseline requirements and aid in making predictions), 
more than 50 features, and greater than 20 unique datasets (and counting). With such a large project, scale becomes a formidable foe when not thought about in advance; with this 
in mind, I made sure that we approached each problem by breaking it down into it's component parts, working with a smaller dataframe or dataset to determine what needs 
to be done, then functionalizing (creating functions from the code written to preliminarily clean and manipulate it) and scaling it up to the entire dataset. I mention all this 
specifically to highlight the thought process we had; I believe what matters most is how you think about things, and then whether or not you can execute them, I believe the 
above code snippet shows both.


# To Infinity...and Beyond!
In this section I want to talk about the current state of the product and what the future state is going to be. Briefly, the core features available are: (1) querying a 
particular city, (2) comparing multiple cities visually in specified categories and (3) viewing historical information relating to specified categories. This encompasses most of 
the core functionality of the product; despite it not seeming like much, each of these features take hours of work to bring to fruition, and due to how we are working through 
things, we might be slow originally on the uptake, but we gain speed and momentum over time as we functionalize our code and then scale across the total number of datapoints.   

Some of the future features include: (1) overlaying data from outside the immediate scope (ex. internet; schooling; health; nightlife / restaurant / activities; etc.), (2) 
making predictions surrounding trends anf allowing the user to view those predictions versuses historical information, (3) an interactive Plotly or Bokeh Dashboard that allows 
users to compare specific features, areas, and other factors on their own, (4) categories and feature scoring as well as overall city and state scoring. With ambitious 
goals also come some foreseeable challenges. One of my major concerns is the ability to make accurate predictions around some of the datapoints -- one of the hardest parts about 
making predictions is that the data needs to meet certian criteria so that we can use probability and statistics to make reliable predicitons, however, when dealing with time 
series data, there are many factors that need to be considered. Just as an example, we need to ensure that the data is homoskedastic and stationary over time, some  of our data 
is neither or those things, so we need to make adjustments. This means we need to decompose our data and analyze the nuances specific to each dataset, which can take time. So 
both execution and timing are major concerns of mine moving forward.

To address feedback, I have had some great feedback from team members as well as having given good feedback as well. To speak specifically about the Data Science team, we have 
had some great communication and stayed on top of our work consistently since the beginning of the project; we have been able to speak openly about any concerns that might arise 
and work fervently to rectify any issues. Despite a lot of issues surrounding team members dropping off or not contributing, we have been able to persevere, overcome adversity 
and still push out quality work; I am very happy with where we are at, and where we are going. I am very optimistic about the future of this product.  

To finish this post, I want to briefly talk about how this project furthers my career goals. Given that I have two degrees in Finance and Economics, this porject ties in 
perfectly with my background and where I want to go with my career. I hope to go and work for a financial research firm writing reports and analyzing companies as well 
as the global economy, so this project is a great case study in what needs to be done to go from zero to a finish project using data science on information relevant to the 
overall global financial system. I think this is a great opportunity to merge my affinity for finance and my obsession with Data Science and Computer Science, and show 
actionable progress in both areas simultaneously.  





